---
tags:
- generated_from_trainer
model-index:
- name: job1
  results: []
---

<!-- This model card has been generated automatically according to the information the Trainer had access to. You
should probably proofread and complete it, then remove this comment. -->

# biobert MultiNERHead finetune

This model is a fine-tuned version of [dmis-lab/biobert-base-cased-v1.2](https://huggingface.co/dmis-lab/biobert-base-cased-v1.2) on the None dataset.
It achieves the following results on the evaluation set:
- Loss: 0.0459
-   Precision: 0.8114
-   Recall: 0.8547
-   F1: 0.8325
-   Number: 52537
- Overall Precision: 0.8114
- Overall Recall: 0.8547
- Overall F1: 0.8325
- Overall Accuracy: 0.9842

## Model description

More information needed

## Intended uses & limitations

More information needed

## Training and evaluation data

More information needed

## Training procedure

### Training hyperparameters

The following hyperparameters were used during training:
- learning_rate: 5e-05
- train_batch_size: 32
- eval_batch_size: 8
- seed: 1
- optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08
- lr_scheduler_type: linear
- num_epochs: 25.0

### Training results

| Training Loss | Epoch | Step  | Validation Loss |   Precision |   Recall |   F1   |   Number | Overall Precision | Overall Recall | Overall F1 | Overall Accuracy |
|:-------------:|:-----:|:-----:|:---------------:|:-----------:|:--------:|:------:|:--------:|:-----------------:|:--------------:|:----------:|:----------------:|
| 0.0604        | 1.0   | 2818  | 0.0461          | 0.7855      | 0.8618   | 0.8219 | 52537    | 0.7855            | 0.8618         | 0.8219     | 0.9824           |
| 0.0439        | 2.0   | 5636  | 0.0459          | 0.8114      | 0.8547   | 0.8325 | 52537    | 0.8114            | 0.8547         | 0.8325     | 0.9842           |
| 0.0351        | 3.0   | 8454  | 0.0508          | 0.8318      | 0.8419   | 0.8368 | 52537    | 0.8318            | 0.8419         | 0.8368     | 0.9839           |
| 0.0273        | 4.0   | 11272 | 0.0557          | 0.8296      | 0.8558   | 0.8425 | 52537    | 0.8296            | 0.8558         | 0.8425     | 0.9847           |
| 0.0213        | 5.0   | 14090 | 0.0609          | 0.8385      | 0.8493   | 0.8439 | 52537    | 0.8385            | 0.8493         | 0.8439     | 0.9847           |
| 0.0168        | 6.0   | 16908 | 0.0623          | 0.8197      | 0.8685   | 0.8434 | 52537    | 0.8197            | 0.8685         | 0.8434     | 0.9844           |
| 0.0135        | 7.0   | 19726 | 0.0680          | 0.8382      | 0.8605   | 0.8492 | 52537    | 0.8382            | 0.8605         | 0.8492     | 0.9850           |
| 0.0108        | 8.0   | 22544 | 0.0715          | 0.8471      | 0.8457   | 0.8464 | 52537    | 0.8471            | 0.8457         | 0.8464     | 0.9846           |
| 0.0082        | 9.0   | 25362 | 0.0773          | 0.8405      | 0.8554   | 0.8479 | 52537    | 0.8405            | 0.8554         | 0.8479     | 0.9848           |
| 0.007         | 10.0  | 28180 | 0.0833          | 0.8430      | 0.8526   | 0.8478 | 52537    | 0.8430            | 0.8526         | 0.8478     | 0.9847           |
| 0.0055        | 11.0  | 30998 | 0.0866          | 0.8415      | 0.8529   | 0.8472 | 52537    | 0.8415            | 0.8529         | 0.8472     | 0.9849           |
| 0.0042        | 12.0  | 33816 | 0.0865          | 0.8414      | 0.8596   | 0.8504 | 52537    | 0.8414            | 0.8596         | 0.8504     | 0.9848           |
| 0.0038        | 13.0  | 36634 | 0.0931          | 0.8347      | 0.8686   | 0.8513 | 52537    | 0.8347            | 0.8686         | 0.8513     | 0.9848           |
| 0.0035        | 14.0  | 39452 | 0.0927          | 0.8482      | 0.8465   | 0.8473 | 52537    | 0.8482            | 0.8465         | 0.8473     | 0.9850           |
| 0.0025        | 15.0  | 42270 | 0.0956          | 0.8467      | 0.8449   | 0.8458 | 52537    | 0.8467            | 0.8449         | 0.8458     | 0.9847           |
| 0.0022        | 16.0  | 45088 | 0.0982          | 0.8455      | 0.8512   | 0.8483 | 52537    | 0.8455            | 0.8512         | 0.8483     | 0.9850           |
| 0.0019        | 17.0  | 47906 | 0.1012          | 0.8501      | 0.8548   | 0.8525 | 52537    | 0.8501            | 0.8548         | 0.8525     | 0.9853           |
| 0.0017        | 18.0  | 50724 | 0.1032          | 0.8502      | 0.8549   | 0.8526 | 52537    | 0.8502            | 0.8549         | 0.8526     | 0.9852           |
| 0.0014        | 19.0  | 53542 | 0.1072          | 0.8487      | 0.8602   | 0.8544 | 52537    | 0.8487            | 0.8602         | 0.8544     | 0.9856           |
| 0.0009        | 20.0  | 56360 | 0.1105          | 0.8522      | 0.8595   | 0.8558 | 52537    | 0.8522            | 0.8595         | 0.8558     | 0.9856           |
| 0.0006        | 21.0  | 59178 | 0.1227          | 0.8485      | 0.8620   | 0.8552 | 52537    | 0.8485            | 0.8620         | 0.8552     | 0.9855           |
| 0.0006        | 22.0  | 61996 | 0.1161          | 0.8507      | 0.8637   | 0.8572 | 52537    | 0.8507            | 0.8637         | 0.8572     | 0.9857           |
| 0.0004        | 23.0  | 64814 | 0.1220          | 0.8523      | 0.8630   | 0.8576 | 52537    | 0.8523            | 0.8630         | 0.8576     | 0.9856           |
| 0.0004        | 24.0  | 67632 | 0.1269          | 0.8527      | 0.8644   | 0.8585 | 52537    | 0.8527            | 0.8644         | 0.8585     | 0.9857           |
| 0.0002        | 25.0  | 70450 | 0.1276          | 0.8551      | 0.8627   | 0.8589 | 52537    | 0.8551            | 0.8627         | 0.8589     | 0.9857           |


### Framework versions

- Transformers 4.26.1
- Pytorch 1.13.1
- Datasets 2.10.1
- Tokenizers 0.13.2